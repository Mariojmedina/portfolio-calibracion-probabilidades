{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnicas de calibración de modelos\n",
    "\n",
    "La calibración de probabilidades busca alinear las probabilidades predichas por un modelo con las frecuencias observadas. Un modelo bien calibrado entrega probabilidades que reflejan la realidad: entre las instancias a las que asigna p=0.7, cerca del 70% debería ser positivo.\n",
    "\n",
    "Este notebook cubre: Histogram Binning, Platt Scaling, Isotonic Regression, Temperature Scaling, y menciona Beta Calibration. Incluye teoría, ventajas/desventajas, ejemplos con código y visualizaciones, además de una comparación final y recomendaciones de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación: librerías y datos sintéticos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = make_classification(n_samples=5000, n_features=20, n_informative=10, n_redundant=5, class_sep=1.0, weights=[0.7,0.3], random_state=42)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "# Modelo base poco calibrado: RandomForest\n",
    "base = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "base.fit(X_tr, y_tr)\n",
    "p_tr = base.predict_proba(X_tr)[:,1]\n",
    "p_te = base.predict_proba(X_te)[:,1]\n",
    "\n",
    "def plot_reliability(ax, y_true, p_pred, label, n_bins=10):\n",
    "    frac_pos, mean_pred = calibration_curve(y_true, p_pred, n_bins=n_bins, strategy='uniform')\n",
    "    ax.plot([0,1],[0,1], '--', color='gray')\n",
    "    ax.plot(mean_pred, frac_pos, marker='o', label=label)\n",
    "    ax.set_xlabel('Prob. media por bin')\n",
    "    ax.set_ylabel('Fracción positiva')\n",
    "    ax.set_title('Reliability plot')\n",
    "    ax.legend()\n",
    "\n",
    "def metrics(y_true, p):\n",
    "    return {\"Brier\": brier_score_loss(y_true, p), \"LogLoss\": log_loss(y_true, p), \"AUC\": roc_auc_score(y_true, p)}\n",
    "\n",
    "base_metrics = metrics(y_te, p_te)\n",
    "base_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Binning (Binning)\n",
    "\n",
    "Idea: dividir el rango de probabilidades en bins y reemplazar cada probabilidad por la frecuencia observada de positivos dentro del bin.\n",
    "- Ventajas: no paramétrico, simple, robusto.\n",
    "- Desventajas: depende de la elección de bins; puede introducir escalones y alta varianza con pocos datos.\n",
    "- Úsalo cuando: dispongas de suficiente validación y quieras una corrección simple y estable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramBinning:\n",
    "    def __init__(self, n_bins=10, strategy='uniform'):\n",
    "        self.n_bins = n_bins\n",
    "        self.strategy = strategy\n",
    "    def fit(self, p, y):\n",
    "        p = np.asarray(p)\n",
    "        y = np.asarray(y)\n",
    "        if self.strategy == 'quantile':\n",
    "            edges = np.quantile(p, np.linspace(0,1,self.n_bins+1))\n",
    "        else:\n",
    "            edges = np.linspace(0,1,self.n_bins+1)\n",
    "        # garantizar monotonía estricta de bordes\n",
    "        edges[0], edges[-1] = 0.0, 1.0\n",
    "        self.edges_ = edges\n",
    "        idx = np.clip(np.digitize(p, edges) - 1, 0, self.n_bins-1)\n",
    "        self.bin_pos_ = np.bincount(idx, weights=y, minlength=self.n_bins)\n",
    "        self.bin_cnt_ = np.bincount(idx, minlength=self.n_bins)\n",
    "        with np.errstate(invalid='ignore', divide='ignore'):\n",
    "            self.bin_rate_ = np.where(self.bin_cnt_>0, self.bin_pos_/self.bin_cnt_, np.nan)\n",
    "        # rellenar nans con interpolación hacia adelante/atrás\n",
    "        valid = ~np.isnan(self.bin_rate_)\n",
    "        self.bin_rate_[~valid] = np.interp(np.flatnonzero(~valid), np.flatnonzero(valid), self.bin_rate_[valid])\n",
    "        return self\n",
    "    def predict(self, p):\n",
    "        p = np.asarray(p)\n",
    "        idx = np.clip(np.digitize(p, self.edges_) - 1, 0, self.n_bins-1)\n",
    "        return self.bin_rate_[idx]\n",
    "\n",
    "hb = HistogramBinning(n_bins=10, strategy='uniform').fit(p_tr, y_tr)\n",
    "p_te_hb = hb.predict(p_te)\n",
    "metrics(y_te, p_te_hb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platt Scaling\n",
    "\n",
    "Ajusta una regresión logística (sigmoide) sobre los scores/probabilidades base para aprender una transformación paramétrica p' = sigmoid(ax + b).\n",
    "- Ventajas: simple, evita sobreajuste con pocos parámetros.\n",
    "- Desventajas: asume forma sigmoidal; puede ser insuficiente si la distorsión no es monotónica.\n",
    "- Úsalo cuando: el descalibrado parece una compresión/expansión sigmoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Usamos logit-transform de las probabilidades base como variable (o directamente los scores de decision_function si el modelo los provee)\n",
    "eps = 1e-6\n",
    "x_tr = np.clip(p_tr, eps, 1-eps).reshape(-1,1)\n",
    "x_te = np.clip(p_te, eps, 1-eps).reshape(-1,1)\n",
    "platt = LogisticRegression(solver='lbfgs')\n",
    "platt.fit(x_tr, y_tr)\n",
    "p_te_platt = platt.predict_proba(x_te)[:,1]\n",
    "metrics(y_te, p_te_platt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotonic Regression\n",
    "\n",
    "Ajusta una función monótona por tramos que mapea scores a probabilidades calibradas.\n",
    "- Ventajas: muy flexible, respeta la monotonía.\n",
    "- Desventajas: puede sobreajustar con pocos datos; requiere validación separada.\n",
    "- Úsalo cuando: el mapeo es no lineal y necesitas flexibilidad con garantía monótona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(p_tr, y_tr)\n",
    "p_te_iso = iso.transform(p_te)\n",
    "metrics(y_te, p_te_iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Scaling\n",
    "\n",
    "Clásico en redes neuronales: divide los logits por un escalar T>0 antes de la sigmoide/softmax. Aquí lo aplicamos sobre log-odds de probabilidades base.\n",
    "- Ventajas: 1 parámetro, estable, mantiene el ranking (AUC).\n",
    "- Desventajas: sólo reescala, no corrige sesgos de base.\n",
    "- Úsalo cuando: el modelo está bien en ranking pero mal en calibración global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def logit(p):\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def fit_temperature(p_val, y_val):\n",
    "    z = logit(p_val)\n",
    "    def obj(t):\n",
    "        T = np.exp(t[0])\n",
    "        p = sigmoid(z / T)\n",
    "        return log_loss(y_val, p)\n",
    "    res = minimize(obj, x0=[0.0], method='L-BFGS-B')\n",
    "    return np.exp(res.x[0])\n",
    "\n",
    "T = fit_temperature(p_tr, y_tr)\n",
    "p_te_temp = sigmoid(logit(p_te)/T)\n",
    "metrics(y_te, p_te_temp), T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización: Reliability plots comparados\n",
    "\n",
    "Comparamos modelo base vs. técnicas calibradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,5))\n",
    "plot_reliability(ax, y_te, p_te, 'Base', n_bins=10)\n",
    "plot_reliability(ax, y_te, p_te_hb, 'Binning', n_bins=10)\n",
    "plot_reliability(ax, y_te, p_te_platt, 'Platt', n_bins=10)\n",
    "plot_reliability(ax, y_te, p_te_iso, 'Isotonic', n_bins=10)\n",
    "plot_reliability(ax, y_te, p_te_temp, 'TempScaling', n_bins=10)\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Modelo':['Base','Binning','Platt','Isotonic','TempScaling'],\n",
    "    'Brier':[\n",
    "        brier_score_loss(y_te, p_te),\n",
    "        brier_score_loss(y_te, p_te_hb),\n",
    "        brier_score_loss(y_te, p_te_platt),\n",
    "        brier_score_loss(y_te, p_te_iso),\n",
    "        brier_score_loss(y_te, p_te_temp),\n",
    "    ],\n",
    "    'LogLoss':[\n",
    "        log_loss(y_te, p_te),\n",
    "        log_loss(y_te, p_te_hb),\n",
    "        log_loss(y_te, p_te_platt),\n",
    "        log_loss(y_te, p_te_iso),\n",
    "        log_loss(y_te, p_te_temp),\n",
    "    ],\n",
    "    'AUC':[\n",
    "        roc_auc_score(y_te, p_te),\n",
    "        roc_auc_score(y_te, p_te_hb),\n",
    "        roc_auc_score(y_te, p_te_platt),\n",
    "        roc_auc_score(y_te, p_te_iso),\n",
    "        roc_auc_score(y_te, p_te_temp),\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta Calibration (explicación)\n",
    "\n",
    "Propone modelar la distribución de probabilidades mediante una familia Beta y aprender un mapeo que corrige sesgo y varianza (Kull et al., 2017). Ofrece mayor flexibilidad que Platt al permitir asimetrías.\n",
    "- Ventajas: flexible, puede mejorar casos donde Platt falla.\n",
    "- Desventajas: más compleja de optimizar; menos soporte en librerías estándar.\n",
    "- Úsalo cuando: necesitas una transformación suave pero más flexible que una sigmoide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros enfoques relevantes\n",
    "- Dirichlet/Vector Scaling (multiclase): extensiones de temperature scaling.\n",
    "- CalibratedClassifierCV (sklearn): envoltorio que hace Platt/Isotonic con validación cruzada.\n",
    "- Post-procesamiento dependiente de umbral y coste (ver otro notebook en este repo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consejos prácticos\n",
    "- Usar un conjunto de validación dedicado para calibrar, diferente al de entrenamiento.\n",
    "- Medir Brier/LogLoss antes y después; el AUC no debería empeorar con métodos monotónicos.\n",
    "- Si hay pocos datos: Platt o Temperature Scaling. Si hay suficiente: Isotonic/Binning.\n",
    "- Verificar estabilidad temporal y por segmentos (drift, subpoblaciones).\n",
   
