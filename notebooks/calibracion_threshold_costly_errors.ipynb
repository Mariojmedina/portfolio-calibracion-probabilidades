{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibración de Probabilidades y Selección de Threshold con Costos Asimétricos\n\nEn problemas de clasificación binaria, la probabilidad predicha debe ser bien calibrada para que represente frecuencias reales. Cuando los costos de error son asimétricos (por ejemplo, el falso negativo es mucho más caro), además de calibrar, debemos optimizar el threshold para minimizar el costo esperado. Este notebook demuestra ambos aspectos de forma práctica y reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n- La calibración de probabilidades garantiza que, entre instancias con p=0.2, ~20% pertenezca a la clase positiva.\n- Sin calibración, la selección de thresholds se vuelve poco confiable y puede subestimar/sobreestimar riesgos.\n- Métricas clave: AUC ROC/PR (discriminación), Brier score (calibración), curvas de confiabilidad, matriz de confusión.\n- Objetivo: entrenar un modelo, evaluar calibración, aplicar Platt scaling e Isotonic Regression, y optimizar threshold bajo costos asimétricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caso práctico: diagnóstico de enfermedad grave (alto costo de FN)\n- Contexto: un test algorítmico apoya diagnóstico de una enfermedad poco prevalente pero grave.\n- Costo asimétrico: Falso Negativo (FN) implica no tratar a un paciente enfermo (costo alto). Falso Positivo (FP) implica derivar a prueba confirmatoria (costo moderado).\n- Ejemplo de costos: C_FN = 50 unidades, C_FP = 1 unidad (ajustables).\n- Meta: minimizar costo esperado con probabilidades calibradas y threshold óptimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulación de datos sintéticos\nGeneramos un dataset desbalanceado (prevalencia ~10%), con 20 variables, para simular el escenario médico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.metrics import (roc_auc_score, roc_curve, precision_recall_curve,\n                            average_precision_score, confusion_matrix, brier_score_loss,\n                            precision_score, recall_score, f1_score)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\", context=\"talk\")\nRANDOM_STATE = 42\nX, y = make_classification(n_samples=6000, n_features=20, n_informative=10, n_redundant=5,\n                           n_clusters_per_class=2, weights=[0.9, 0.1], class_sep=1.2,\n                           flip_y=0.01, random_state=RANDOM_STATE)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, stratify=y, random_state=RANDOM_STATE\n)\nprint('Prevalencia train:', y_train.mean().round(3), '| Prevalencia test:', y_test.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del modelo (Logistic Regression)\nEntrenamos una logística con estandarización. Guardamos probabilidades crudas (sin calibrar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_clf = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('lr', LogisticRegression(max_iter=1000, class_weight=None, random_state=RANDOM_STATE))\n])\nbase_clf.fit(X_train, y_train)\np_base = base_clf.predict_proba(X_test)[:,1]\nprint('AUC (base, sin calibrar):', roc_auc_score(y_test, p_base).round(3))\nprint('Brier (base):', brier_score_loss(y_test, p_base).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Métricas iniciales y matriz de confusión (threshold 0.5)\nCalculamos precisión, recall, F1 y la matriz de confusión con threshold estándar 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(cm, labels=('Neg','Pos'), title='Matriz de confusión'):\n    plt.figure(figsize=(5,4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=labels, yticklabels=labels)\n    plt.ylabel('Real'); plt.xlabel('Predicho'); plt.title(title); plt.tight_layout(); plt.show()\n\nthr_default = 0.5\ny_pred_05 = (p_base >= thr_default).astype(int)\ncm_05 = confusion_matrix(y_test, y_pred_05)\nplot_confusion(cm_05, title=f'Matriz de confusión (thr={thr_default})')\nprint('Precision:', precision_score(y_test, y_pred_05).round(3))\nprint('Recall:', recall_score(y_test, y_pred_05).round(3))\nprint('F1:', f1_score(y_test, y_pred_05).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ROC y PR Curve\nLas curvas ROC/PR evalúan la capacidad de discriminación. En datasets desbalanceados, el AUPRC (Average Precision) aporta contexto adicional al AUC ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\nfpr, tpr, roc_th = roc_curve(y_test, p_base)\nauc_roc = roc_auc_score(y_test, p_base)\nplt.figure(figsize=(6,5)); plt.plot(fpr, tpr, label=f'AUC={auc_roc:.3f}')\nplt.plot([0,1],[0,1],'k--', alpha=0.6)\nplt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve (base)'); plt.legend(); plt.tight_layout(); plt.show()\n# PR\nprec, rec, pr_th = precision_recall_curve(y_test, p_base)\nauprc = average_precision_score(y_test, p_base)\nplt.figure(figsize=(6,5)); plt.plot(rec, prec, label=f'AP={auprc:.3f}')\nplt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR Curve (base)'); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calibración: Platt scaling e Isotonic Regression\nUsamos CalibratedClassifierCV sobre el clasificador base (validez out-of-sample vía validación cruzada). Comparamos Brier score y curvas de confiabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\nplatt = CalibratedClassifierCV(estimator=clone(base_clf), method='sigmoid', cv=5)\nplatt.fit(X_train, y_train)\np_platt = platt.predict_proba(X_test)[:,1]\niso = CalibratedClassifierCV(estimator=clone(base_clf), method='isotonic', cv=5)\niso.fit(X_train, y_train)\np_iso = iso.predict_proba(X_test)[:,1]\nprint('AUC base:', roc_auc_score(y_test, p_base).round(3))\nprint('AUC platt:', roc_auc_score(y_test, p_platt).round(3))\nprint('AUC iso:  ', roc_auc_score(y_test, p_iso).round(3))\nprint('Brier base:', brier_score_loss(y_test, p_base).round(3))\nprint('Brier platt:', brier_score_loss(y_test, p_platt).round(3))\nprint('Brier iso:  ', brier_score_loss(y_test, p_iso).round(3))\nfrom sklearn.calibration import calibration_curve as _cc\ndef plot_reliability(y_true, probs_list, labels, n_bins=10, title='Curva de confiabilidad'):\n    plt.figure(figsize=(6,5))\n    for p, lab in zip(probs_list, labels):\n        frac_pos, mean_pred = _cc(y_true, p, n_bins=n_bins, strategy='uniform')\n        plt.plot(mean_pred, frac_pos, marker='o', label=lab)\n    plt.plot([0,1],[0,1],'k--', alpha=0.6)\n    plt.xlabel('Probabilidad predicha promedio')\n    plt.ylabel('Fracción positiva')\n    plt.title(title)\n    plt.legend(); plt.tight_layout(); plt.show()\nplot_reliability(y_test, [p_base, p_platt, p_iso], ['Base','Platt','Isotónica'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Análisis de thresholds y costo esperado\nDefinimos: Costo = C_FP*FP + C_FN*FN. Exploramos thresholds y elegimos el que minimiza el costo (usamos probabilidades isotónicas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_summary(y_true, p, thr, c_fp=1.0, c_fn=50.0):\n    y_hat = (p >= thr).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n    total = c_fp*fp + c_fn*fn\n    return {'thr':thr, 'fp':int(fp), 'fn':int(fn), 'tp':int(tp), 'tn':int(tn), 'cost': float(total)}\nthresholds = np.linspace(0.01, 0.99, 99)\ndf_cost_iso = pd.DataFrame([cost_summary(y_test, p_iso, t, c_fp=1.0, c_fn=50.0) for t in thresholds])\ndf_cost_iso.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\nplt.plot(df_cost_iso['thr'], df_cost_iso['cost'], label='Costo total (Isotónica)')\nplt.axvline(0.5, color='grey', linestyle='--', alpha=0.6, label='thr=0.5')\nt_opt = df_cost_iso.loc[df_cost_iso['cost'].idxmin(), 'thr']\nc_opt = df_cost_iso['cost'].min()\nplt.axvline(t_opt, color='crimson', linestyle='--', label=f'thr óptimo={t_opt:.3f}')\nplt.xlabel('Threshold'); plt.ylabel('Costo total'); plt.title('Costo vs Threshold')\nplt.legend(); plt.tight_layout(); plt.show()\nprint(f'Threshold óptimo (mínimo costo): {t_opt:.3f} | Costo: {c_opt:.1f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusión y métricas: thr=0.5 vs thr óptimo\nComparamos desempeño con probabilidades isotónicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_iso_05 = (p_iso >= 0.5).astype(int)\ny_pred_iso_opt = (p_iso >= t_opt).astype(int)\ncm_iso_05 = confusion_matrix(y_test, y_pred_iso_05)\ncm_iso_opt = confusion_matrix(y_test, y_pred_iso_opt)\nplot_confusion(cm_iso_05, title='Isotónica - thr=0.5')\nplot_confusion(cm_iso_opt, title=f'Isotónica - thr óptimo={t_opt:.3f}')\nprint('Precision@0.5:', precision_score(y_test, y_pred_iso_05).round(3), '| Recall@0.5:', recall_score(y_test, y_pred_iso_05).round(3))\nprint('Precision@opt:', precision_score(y_test, y_pred_iso_opt).round(3), '| Recall@opt:', recall_score(y_test, y_pred_iso_opt).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusión\n- La calibración (Platt/Isotónica) reduce el Brier score y mejora la confiabilidad.\n- Con costos asimétricos, es subóptimo usar thr=0.5; debe elegirse el threshold que minimiza el costo esperado en el conjunto de validación/test.\n- Flujo recomendado: (1) entrenar, (2) calibrar probabilidades, (3) cuantificar costo FN/FP, (4) optimizar threshold según costo, (5) monitorear en producción."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
